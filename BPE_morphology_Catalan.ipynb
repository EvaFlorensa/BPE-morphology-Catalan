{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Obtaining the text from an URL\n",
        "import urllib.request\n",
        "\n",
        "url = \"http://research.ics.aalto.fi/cog/data/udhr/txt/cln.txt\"\n",
        "\n",
        "with urllib.request.urlopen(url) as response:\n",
        "    text = response.read().decode('utf-8') # Read document and decode it with UTF-8\n",
        "\n",
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTfuoMyGujOh",
        "outputId": "6a5c3ae6-e7d1-4a8c-a3dd-4eef31ac9721"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base for the tokenizers"
      ],
      "metadata": {
        "id": "emlno7hb-DI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids, counts=None): # It takes a list of integers and returns a dictionary of counts of consecutive paris.\n",
        "    counts = {} if counts is None else counts\n",
        "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx): # In the list of integers (ids), it replaces the consecutive occurrences of paris with the new integer token idx.\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "\n",
        "# Base class for the tokenizers\n",
        "\n",
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self): # By default the vocab size is 256 (all bytes), without merges and patterns.\n",
        "        self.merges = {} # (int, int) -> int\n",
        "        self.pattern = \"\" # str\n",
        "        self.special_tokens = {} # str -> int\n",
        "        self.vocab = self._build_vocab() # int -> bytes\n",
        "\n",
        "    def _build_vocab(self): # Building the vocabulary from the merges (deterministically derived).\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for (p0, p1), idx in self.merges.items():\n",
        "            vocab[idx] = vocab[p0] + vocab[p1]\n",
        "        for special, idx in self.special_tokens.items():\n",
        "            vocab[idx] = special.encode(\"utf-8\")\n",
        "        return vocab\n"
      ],
      "metadata": {
        "id": "qczZdw1Hl5G7"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic tokenizer"
      ],
      "metadata": {
        "id": "C57BdK2K-gt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating the class for the basic tokenizer, taking as a base the class Tokenizer that we have created before.\n",
        "\n",
        "class BasicTokenizer(Tokenizer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False): # Creating the training step.\n",
        "        assert vocab_size >= 256\n",
        "        num_merges = vocab_size - 256\n",
        "\n",
        "        # Input text for preprocessing.\n",
        "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
        "        ids = list(text_bytes) # list of integers\n",
        "\n",
        "        # Iteratively merge the most common pairs to create new tokens\n",
        "        merges = {} # (int, int) -> int\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n",
        "        for i in range(num_merges):\n",
        "            # count the number of times every consecutive pair appears\n",
        "            stats = get_stats(ids)\n",
        "            # find the pair with the highest count\n",
        "            pair = max(stats, key=stats.get)\n",
        "            # mint a new token: assign it the next available id\n",
        "            idx = 256 + i\n",
        "            # replace all occurrences of pair in ids with idx\n",
        "            ids = merge(ids, pair, idx)\n",
        "            # save the merge\n",
        "            merges[pair] = idx\n",
        "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
        "            # print it\n",
        "            if verbose:\n",
        "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
        "\n",
        "        # Save class variables for later using/printing them.\n",
        "        self.merges = merges\n",
        "        self.vocab = vocab"
      ],
      "metadata": {
        "id": "4Y3UD0io-f2c"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the tokenizer with the new class BasicTokenizer.\n",
        "basic_tokenizer = BasicTokenizer()\n",
        "training_text = text\n",
        "vocab_size = 456  # Vocabulary size (256 bytes + 200 merges)\n",
        "basic_tokenizer.train(training_text, vocab_size)"
      ],
      "metadata": {
        "id": "C2b1stTp-rJC"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising the results (subwords) obtained from the first 20 merges.\n",
        "for index, (k, v) in enumerate(basic_tokenizer.vocab.items()):\n",
        "    if 256 <= index < 276:\n",
        "        print(f\"{k} => {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmooM1wbPk5j",
        "outputId": "093e1d04-54af-442d-c611-306559a7fdaa"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256 => b'a '\n",
            "257 => b's '\n",
            "258 => b'e '\n",
            "259 => b'en'\n",
            "260 => b'er'\n",
            "261 => b' d'\n",
            "262 => b'ci'\n",
            "263 => b're'\n",
            "264 => b'on'\n",
            "265 => b'al'\n",
            "266 => b'i '\n",
            "267 => b't '\n",
            "268 => b', '\n",
            "269 => b'ta'\n",
            "270 => b'el'\n",
            "271 => b'. '\n",
            "272 => b'la '\n",
            "273 => b'\\xc3\\xb3'\n",
            "274 => b'ti'\n",
            "275 => b'per'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regex tokenizer (adapted)"
      ],
      "metadata": {
        "id": "D0Uaw2Tl-GNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating the class for the regex tokenizer, taking as a base the class Tokenizer that we have created before.\n",
        "\n",
        "import regex as re\n",
        "\n",
        "# Pattern for text spliting, based on the GPT4 pattern, but adapted to Catalan.\n",
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[\\w]+|'l'|'ls|'m'|'t'|'ns|-l.+|n.+|s.+|l'|d'|n'|s'|t')|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "\n",
        "class RegexTokenizer(Tokenizer):\n",
        "\n",
        "    def __init__(self, pattern=None):\n",
        "        super().__init__()\n",
        "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern # Here we say we want to use our specific pattern.\n",
        "        self.compiled_pattern = re.compile(self.pattern)\n",
        "        self.special_tokens = {}\n",
        "        self.inverse_special_tokens = {}\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "        assert vocab_size >= 256\n",
        "        num_merges = vocab_size - 256\n",
        "\n",
        "        text_chunks = re.findall(self.compiled_pattern, text) # Split the text up into text chunks, using the pattern created before.\n",
        "\n",
        "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks] # Input text preprocessing\n",
        "\n",
        "        # Iteratively merge the most common pairs to create the new tokens\n",
        "        merges = {} # (int, int) -> int\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
        "        for i in range(num_merges):\n",
        "            # Count the number of times that appears every consecutive pair.\n",
        "            stats = {}\n",
        "            for chunk_ids in ids:\n",
        "                get_stats(chunk_ids, stats)\n",
        "            pair = max(stats, key=stats.get) # find the pair with the highest count\n",
        "            idx = 256 + i # assigning to the new token the next available id.\n",
        "            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
        "            merges[pair] = idx # save the merge\n",
        "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
        "            if verbose:\n",
        "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
        "\n",
        "        # Save the class variables for later using/printing them.\n",
        "        self.merges = merges\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def register_special_tokens(self, special_tokens):\n",
        "        # Special_tokens is a dictionary of str -> int\n",
        "        self.special_tokens = special_tokens\n",
        "        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n"
      ],
      "metadata": {
        "id": "-iFBn-zWmmDw"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the tokenizer with the new class RegexTokenizer.\n",
        "regex_tokenizer =  RegexTokenizer()\n",
        "training_text = text\n",
        "vocab_size = 456 # Vocabulary size (256 bytes + 200 merges)\n",
        "regex_tokenizer.train(training_text, vocab_size)"
      ],
      "metadata": {
        "id": "AH8zgRnVx5kH"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising the results (subwords) obtained from the first 20 merges.\n",
        "for index, (k, v) in enumerate(regex_tokenizer.vocab.items()):\n",
        "    if 256 <= index < 276:\n",
        "        print(f\"{k} => {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDpK_y6UV28s",
        "outputId": "cc56b11c-07cc-4ad9-ef0b-baf7f91d400b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256 => b' d'\n",
            "257 => b'en'\n",
            "258 => b' l'\n",
            "259 => b'es'\n",
            "260 => b'er'\n",
            "261 => b' a'\n",
            "262 => b' p'\n",
            "263 => b'ci'\n",
            "264 => b' i'\n",
            "265 => b'ta'\n",
            "266 => b' de'\n",
            "267 => b'on'\n",
            "268 => b're'\n",
            "269 => b' s'\n",
            "270 => b'al'\n",
            "271 => b' la'\n",
            "272 => b' c'\n",
            "273 => b'\\xc3\\xb3'\n",
            "274 => b'ti'\n",
            "275 => b' e'\n"
          ]
        }
      ]
    }
  ]
}
